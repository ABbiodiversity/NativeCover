@Article{DeLancey2020,
AUTHOR = {DeLancey, Evan R. and Simms, John F. and Mahdianpari, Masoud and Brisco, Brian and Mahoney, Craig and Kariyeva, Jahan},
TITLE = {Comparing Deep Learning and Shallow Learning for Large-Scale Wetland Classification in Alberta, Canada},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {2},
URL = {https://www.mdpi.com/2072-4292/12/1/2},
ISSN = {2072-4292},
ABSTRACT = {Advances in machine learning have changed many fields of study and it has also drawn attention in a variety of remote sensing applications. In particular, deep convolutional neural networks (CNNs) have proven very useful in fields such as image recognition; however, the use of CNNs in large-scale remote sensing landcover classifications still needs further investigation. We set out to test CNN-based landcover classification against a more conventional XGBoost shallow learning algorithm for mapping a notoriously difficult group of landcover classes, wetland class as defined by the Canadian Wetland Classification System. We developed two wetland inventory style products for a large (397,958 km2) area in the Boreal Forest region of Alberta, Canada, using Sentinel-1, Sentinel-2, and ALOS DEM data acquired in Google Earth Engine. We then tested the accuracy of these two products against three validation data sets (two photo-interpreted and one field). The CNN-generated wetland product proved to be more accurate than the shallow learning XGBoost wetland product by 5%. The overall accuracy of the CNN product was 80.2% with a mean F1-score of 0.58. We believe that CNNs are better able to capture natural complexities within wetland classes, and thus may be very useful for complex landcover classifications. Overall, this CNN framework shows great promise for generating large-scale wetland inventory data and may prove useful for other landcover mapping applications.},
DOI = {10.3390/rs12010002}
}

@article{DeLancey2019,
    doi = {10.1371/journal.pone.0218165},
    author = {DeLancey, Evan Ross AND Kariyeva, Jahan AND Bried, Jason T. AND Hird, Jennifer N.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Large-scale probabilistic identification of boreal peatlands using Google Earth Engine, open-access satellite data, and machine learning},
    year = {2019},
    month = {06},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pone.0218165},
    pages = {1-23},
    abstract = {Freely-available satellite data streams and the ability to process these data on cloud-computing platforms such as Google Earth Engine have made frequent, large-scale landcover mapping at high resolution a real possibility. In this paper we apply these technologies, along with machine learning, to the mapping of peatlands–a landcover class that is critical for preserving biodiversity, helping to address climate change impacts, and providing ecosystem services, e.g., carbon storage–in the Boreal Forest Natural Region of Alberta, Canada. We outline a data-driven, scientific framework that: compiles large amounts of Earth observation data sets (radar, optical, and LiDAR); examines the extracted variables for suitability in peatland modelling; optimizes model parameterization; and finally, predicts peatland occurrence across a large boreal area (397, 958 km2) of Alberta at 10 m spatial resolution (equalling 3.9 billion pixels across Alberta). The resulting peatland occurrence model shows an accuracy of 87% and a kappa statistic of 0.57 when compared to our validation data set. Differentiating peatlands from mineral wetlands achieved an accuracy of 69% and kappa statistic of 0.37. This data-driven approach is applicable at large geopolitical scales (e.g., provincial, national) for wetland and landcover inventories that support long-term, responsible resource management.},
    number = {6},

}
